# Questions

## How precise can tool calling be?

* How is tool calling triggered?
* Are there benchmarks around tool call accuracy?
* Empirical results, how many tools are optimal, with respect to context?
* How dissimilar do descriptions need to be?
* Do some LLMs provide a precise tool call structure, ie. how tools should be defined?

### How is tool calling triggered?

Two mechanisms depending on the provider:

**API-native (OpenAI, Anthropic, Google):** Tool definitions are passed as structured
JSON alongside the request. The API layer injects them into a system prompt template.
The model generates a structured JSON object (with `name` and `input`/`arguments`)
as regular text output, which the API parses and returns as a typed `tool_use` content
block. The model does *not* call the function -- it outputs a structured request that
your code must intercept, execute, and feed back as a `tool_result`.

**Open-source/fine-tuned:** Models are fine-tuned with special tokens like
`<tool_call>`, `</tool_call>` that demarcate tool invocation boundaries. The tokenizer
vocabulary is expanded to accommodate these. At inference, a parser extracts the
structured call from between the delimiters.

**Takeaway:** You are building the orchestrator. The model just produces JSON that
says "call this tool with these args." Your agent loop: (1) detect `tool_use` stop
reason, (2) extract name + args, (3) execute, (4) feed result back, (5) loop until
final text response.

### Benchmarks around tool call accuracy

**Berkeley Function Calling Leaderboard (BFCL)** is the primary benchmark (now v4).
Top BFCL v4 scores (late 2025): GLM-4.5 ~70.9%, Claude Opus 4.1 ~70.4%, Claude
Sonnet 4 ~70.3%, GPT-5 ~59.2%. Best models achieve only ~70% on the full evaluation.
Models excel at single-turn isolated calls but struggle with context management and
long conversations.

**MCPMark** tests realistic CRUD workflows (~16 execution turns, ~17 tool calls per
task). Results are dramatically lower: GPT-5 Medium 52.6% Pass@1, Claude Opus 4.1
29.9%, Claude Sonnet 4 28.1%.

**Takeaway:** Do not assume tool calling is reliable. Build retries (2-3 attempts),
validate tool arguments against schema before execution, and add graceful error handling.

### How many tools are optimal?

No single magic number, but converging data:

* **< 30 tools:** Success rates above 90%, model reliably distinguishes tools
* **30-70 tools:** Accuracy clusters as semantic overlap increases, noticeable degradation
* **100+ tools:** Retrieval precision drops sharply; naive selection drops from ~90% to ~14% at 1,100 tools (RAG-MCP paper, arxiv 2505.03275)
* Each tool definition uses ~100-300 tokens; 50 tools = 5,000-15,000 tokens before any conversation
* OpenAI hard limit: 128 tools; recommended: fewer than ~100 tools, fewer than ~20 arguments per tool

**Takeaway:** Start with 3-5 essential tools. If you need 20-30+, implement a two-stage
approach: retrieval step (embedding similarity) to select the 5-10 most relevant tools
per query, then pass only those to the LLM.

### How dissimilar do tool descriptions need to be?

LLMs select tools via semantic similarity between user query and descriptions. When
descriptions overlap, models "may call the wrong one or hesitate to call any at all."

**Anthropic's guidance:** Provide extremely detailed descriptions (3-4+ sentences per
tool). Explain what it does, *when* to use it, *when not* to use it, what each parameter
means, and caveats.

**Practical strategies:**

* Give each tool a unique verb (`search_files` vs `read_file` vs `list_directory` -- not `find_files` vs `search_files` vs `locate_files`)
* Include negative examples: "Use this to search file contents. Do NOT use for listing directories -- use list_directory instead."
* If two tools are semantically close, explicitly mention the other tool in each description

### Do LLMs provide a precise tool call structure?

Yes, both major providers define exact schemas. Key differences:

| Aspect | Anthropic | OpenAI |
|--------|-----------|--------|
| Schema field | `input_schema` | `parameters` (nested under `function`) |
| Strict mode | `strict: true` on tool def | `strict: true` inside `function` |
| Tool name regex | `^[a-zA-Z0-9_-]{1,64}$` | Similar |
| Response format | `tool_use` block with `id`, `name`, `input` | `tool_calls` array with `id`, `function.name`, `function.arguments` (JSON string) |
| Max description | Not specified (3-4+ sentences) | 1,024 chars (Azure) |

**Strict mode** guarantees the output conforms exactly to your JSON schema -- eliminates
an entire class of invalid-argument errors.

**Takeaway for Go:** Define a common internal `Tool` struct, then write
provider-specific serializers. Always enable `strict: true` when available.

---

## Tool call guardrail questions?

* What are some guardrails and safety measures around tool call?
* Is there a way to give a tool call confidence score; as a security measure, e.g. require a high confidence score in the answer (token logprobs?)

### Guardrails and safety measures

1. **Permission systems (allowlist/denylist):** Define which tools the agent can call and under what conditions. Map tool names to permission levels (always-allow, ask-user, deny).
2. **Input/output validation:** Validate parameter types, value ranges, and unexpected fields before execution. Validate tool output before feeding it back to the LLM.
3. **Rate limiting and loop detection:** Set max tool calls per turn, max token budget. Detect cycles (same tool, same args, repeatedly).
4. **Tiered human-in-the-loop:** Read-only tools auto-approve; mutation tools may require confirmation; destructive tools always require explicit approval.
5. **Prompt injection defense:** Tool outputs can contain adversarial content. Clearly delimit results (e.g., XML tags) and instruct model to treat tool outputs as untrusted data.
6. **Audit logging:** Log every tool call with arguments, reasoning, result, and timestamp.

### Confidence scores for tool calls

**Token logprobs:** Most APIs can return log probabilities for generated tokens (logprob = `log(p)`). Average the logprobs across the tool call's name and argument tokens for an aggregate score.

Practical thresholds:
* **logprob > -0.1** (p > 0.90): high confidence, auto-execute
* **-0.5 < logprob < -0.1** (0.60 < p < 0.90): medium, show warning
* **logprob < -0.5** (p < 0.60): low, require explicit approval

**Limitations:** Logprobs measure self-consistency, not factual correctness. Not all providers expose logprobs for tool-call tokens (OpenAI does; Claude currently does not).

**Alternative approaches:**

* **Self-verification loop:** Second (cheaper) model or prompt asks "Is this tool call correct and safe?"
* **N-of-M voting:** Sample N times at temperature > 0, only execute if M agree on same tool + args
* **Anomaly detection:** Track tool call distribution; flag unusual patterns

---

## What are better ways to serialize structured data in an LLM context?

* Anything better than JSON? How and why?

### YAML

Uses ~30% fewer tokens than JSON (no braces, brackets, key quotes, commas).
Outperformed JSON on accuracy for 2/3 models tested (improvingagents.com).

### TOON (Token-Oriented Object Notation)

Purpose-built for LLMs ([toonformat.dev](https://toonformat.dev/)). Uses indentation
instead of braces, arrays of uniform objects written in tabular form with headers.

```
// JSON (37 tokens)
[{"name":"Alice","age":30},{"name":"Bob","age":25}]

// TOON (~22 tokens)
[2]{name,age}
  Alice	30
  Bob	25
```

~40% fewer tokens than JSON, 74% accuracy vs JSON's 70% across 209 data retrieval
questions. Round-trips losslessly to/from JSON. Go implementation available.

### XML

Claude internally uses XML-style tags for tool calling. Advantages for wrapping code:
clear start/end tags, minor malformation is recoverable (unlike JSON where a missing
comma is fatal). More verbose for data payloads though.

### Practical recommendation

* **Tool definitions/structure:** XML-style tags or custom format
* **Data payloads in context (RAG chunks, DB results):** YAML or TOON for token savings
* **Tool call arguments:** If you must use JSON (most APIs require it), keep schemas flat with scalar types

---

## Escaping JSON

* JSON escape?

### The core problem

LLMs perform worse when returning code inside JSON tool calls vs plain markdown
(aider benchmark, 133 coding exercises). When producing JSON, the model must
simultaneously generate correct code *and* correctly escape it (`\n`, `\"`, `\\`,
`\t`), which consumes the model's per-token computation budget.

### Common failures

* Unescaped newlines/tabs in string values (invalid JSON)
* Double-escaping (`\\n` instead of `\n`)
* Unescaped quotes inside strings
* Trailing commas (valid JS, not JSON)

### Mitigation strategies

1. **Lenient JSON parsing:** Don't use strict parsing for LLM output. Handle trailing commas, single quotes, unescaped newlines, missing closing braces.
2. **Avoid code inside JSON strings:** Design schemas so code is returned outside the JSON structure (e.g., metadata JSON + separate markdown code block).
3. **XML-style delimiters for code:** `<tool name="write_file" path="/foo.go">...code, no escaping...</tool>` -- sidesteps JSON escaping entirely.
4. **Keep schemas flat and scalar:** Avoid nested objects/arrays in tool parameters.
5. **Structured output mode** (`strict: true`): Guarantees valid JSON at the grammar level, but does *not* improve code quality inside strings.
6. **Post-hoc repair:** Progressively fix trailing commas, missing braces, unescaped control chars.

---

## Ways to secure command execution on a machine

* if an llm wants can execute random commands, how can we protect the environment?

### Defense layers (lightest to heaviest)

1. **No shell interpretation:** Use `exec.Command("git", "status")` directly (Go's `os/exec`), not `sh -c "..."`. Arguments passed as argv array, eliminating shell injection. Lose pipes/redirects/globs -- but that's the point.
2. **Command allowlisting + argument validation:** Explicit list of allowed commands with argument pattern validation. Beware blind spots: command substitution (`$(...)`, backticks), pipes, redirects, semicolons can bypass naive allowlists. Must parse commands properly or disallow shell interpretation.
3. **Filesystem restrictions:** `chroot`, bind mounts, or containers to restrict directory access. Set `cmd.Dir` in Go.
4. **Linux namespaces + seccomp:** Create child processes with restricted mount, PID, and network namespaces via `syscall.SysProcAttr{Cloneflags: ...}`. Add seccomp filters to restrict allowed syscalls.
5. **Container isolation:** Run commands inside Docker containers or [gVisor](https://gvisor.dev/) sandbox (user-space kernel, written in Go). Read-only filesystem, no network, limited CPU/memory, timeouts.
6. **VM isolation:** Firecracker/QEMU microVMs for near-total isolation (~100ms startup). Services like E2B provide this as a service.

### Additional runtime protections

* Hard timeout on every command (e.g., 60s)
* Cap output size (don't let stdout fill memory)
* Drop all unnecessary Linux capabilities
* Run as non-root with minimal permissions
* Disable network for commands that don't need it

**Practical recommendation:** Layer 1 (no shell) as baseline + Layer 2 (allowlisting)
as policy + Layer 5 (Docker) for high-risk commands.

---

## What are ways to improve provenance information, e.g. in a RAG system?

* unique id, pdf, link-to-page?

### Chunk-level metadata

Every chunk should carry structured metadata: chunk ID, document ID, document name,
page number, bounding box (spatial location on page), section title, chunk index,
source URL, ingestion timestamp.

### Deterministic chunk IDs

Generate IDs from content + position (e.g., SHA-256 of `docID:page:offset:content`,
truncated). Re-indexing same document produces same IDs -- important for cache stability
and deduplication.

### PDF deep linking

PDF viewers support fragment identifiers: `/documents/report.pdf#page=5`. For finer
granularity, use PDF.js with custom highlighting based on bounding box coordinates.

### Inline citation anchors

Insert lightweight markers at indexing time: `<c:doc123:p5:s2/>`. Instruct the LLM to
preserve them in output. Parse and resolve to full citations with links after generation.

### Layout-aware parsing

Use parsers like Docling, Apache Tika, or Reducto that extract text *and* spatial
positions (bounding boxes) for every block, table, and figure. Enables "evidence pins"
-- clickable highlights showing exactly which part of the source document was used.

### Citation verification

After generation, verify every citation the model produces actually exists in the
retrieved chunks. Catches hallucinated citations -- a known problem where models invent
plausible references.

### Multi-level provenance hierarchy

* **Document level:** ID, title, author, date, URL
* **Section level:** heading, section number
* **Chunk level:** page, bounding box, paragraph index
* **Sentence level:** character offsets within the chunk

Lets you provide citations at whatever granularity the user needs.

---

## References

### Tool Calling Mechanics & Benchmarks

* https://martinuke0.github.io/posts/2026-01-07-the-anatomy-of-tool-calling-in-llms-a-deep-dive/
* https://huggingface.co/learn/agents-course/en/unit1/messages-and-special-tokens
* https://platform.claude.com/docs/en/agents-and-tools/tool-use/implement-tool-use
* https://gorilla.cs.berkeley.edu/leaderboard.html
* https://www.klavis.ai/blog/function-calling-and-agentic-ai-in-2025-what-the-latest-benchmarks-tell-us-about-model-performance
* https://openreview.net/forum?id=2GmDdhBdDk
* https://blog.quotientai.co/evaluating-tool-calling-capabilities-in-large-language-models-a-literature-review/

### Tool Count & Description Design

* https://arxiv.org/abs/2505.03275
* https://demiliani.com/2025/09/04/model-context-protocol-and-the-too-many-tools-problem/
* https://www.useparagon.com/learn/rag-best-practices-optimizing-tool-calling/
* https://achan2013.medium.com/how-many-tools-functions-can-an-ai-agent-has-21e0a82b7847
* https://arxiv.org/html/2510.19791
* https://platform.openai.com/docs/guides/function-calling
* https://cookbook.openai.com/examples/o-series/o3o4-mini_prompting_guide/
* https://composio.dev/blog/claude-function-calling-tools
* https://medium.com/@laurentkubaski/openai-tool-schema-explained-05a5ce0e80f8

### Guardrails & Confidence Scoring

* https://docs.litellm.ai/docs/proxy/guardrails/tool_permission
* https://stevekinney.com/courses/ai-development/claude-code-permissions
* https://www.datadoghq.com/blog/llm-guardrails-best-practices/
* https://github.com/NVIDIA-NeMo/Guardrails
* https://sombrainc.com/blog/llm-security-risks-2026
* https://ericjinks.com/blog/2025/logprobs/
* https://gautam75.medium.com/unlocking-llm-confidence-through-logprobs-54b26ed1b48a
* https://github.com/VATBox/llm-confidence
* https://arxiv.org/html/2512.03816v1
* https://medium.com/@vatvenger/confidence-unlocked-a-method-to-measure-certainty-in-llm-outputs-1d921a4ca43c

### Serialization Formats

* https://www.improvingagents.com/blog/best-nested-data-format/
* https://toonformat.dev/
* https://medium.com/@michael.hannecke/beyond-json-picking-the-right-format-for-llm-pipelines-b65f15f77f7d
* https://apidog.com/blog/toon-vs-json-for-llms/
* https://david-gilbertson.medium.com/llm-output-formats-why-json-costs-more-than-tsv-ebaf590bd541
* https://arcfu.com/post/claude-json-solution/

### JSON Escaping

* https://aider.chat/2024/08/14/code-in-json.html
* https://github.com/block/goose/issues/2892
* https://boundaryml.com/blog/schema-aligned-parsing

### Command Execution Security

* https://dida.do/blog/setting-up-a-secure-python-sandbox-for-llm-agents
* https://amirmalik.net/2025/03/07/code-sandboxes-for-llm-ai-agents
* https://www.innoq.com/en/blog/2025/12/dev-sandbox/
* https://mfyz.com/claude-code-allowlist-command-substitution-bypass
* https://ricardoanderegg.com/posts/control-shell-permissions-llm-codex/
* https://gvisor.dev/docs/
* https://arxiv.org/pdf/2510.21236

### RAG Provenance

* https://www.tensorlake.ai/blog/rag-citations
* https://vipulmshah.medium.com/layout-aware-rag-with-evidence-pins-building-clickable-citations-for-pdfs-using-docling-neo4j-5305769759f0
* https://docs.reducto.ai/parsing/block-level-citations
* https://www.multimodal.dev/post/how-to-chunk-documents-for-rag
* https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-enrichment-phase
* https://users.ics.forth.gr/~tzitzik/publications/Tzitzikas_2025-09-TPDL-KGRAG.pdf
